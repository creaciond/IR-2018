{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2: _Парсинг Авито_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для работы с вебом\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "# для чистки текста\n",
    "import re\n",
    "import html\n",
    "\n",
    "# для работы с файловой системой\n",
    "import os\n",
    "\n",
    "# для составления таблицы с метаинформацией\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачать страницу и обработать её содержимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "yesterday.strftime(\"%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta(link, soup):\n",
    "    page_meta = {\n",
    "        \"item\": \"\",\n",
    "        \"id\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"seller\": \"\",\n",
    "        \"address\": \"\"\n",
    "    }\n",
    "    reg_date = re.compile(\"размещено (.*)?  \")\n",
    "    reg_id = re.compile(\"№ ([0-9]+)\")\n",
    "    \n",
    "    ad_item_str = link[46:]\n",
    "    ad_metadata = soup.select(\"div.title-info-metadata-item\")[0].text\n",
    "    ad_item_id = re.search(reg_id, ad_metadata).group(1)\n",
    "    \n",
    "    ad_date = re.search(reg_date, ad_metadata).group(1)\n",
    "    if \"вчера\" in ad_date:\n",
    "        yesterday = datetime.now() - timedelta(days=1)\n",
    "        ad_date = re.sub(\"вчера\", yesterday.strftime(\"%d\").lstrip(\"0\") + \" октября\", ad_date)\n",
    "    elif \"сегодня\" in ad_date:\n",
    "        today = datetime.now()\n",
    "        ad_date = re.sub(\"сегодня\", today.strftime(\"%d\").lstrip(\"0\") + \" октября\", ad_date)\n",
    "    ad_seller = soup.find(\"div\", attrs={\"class\": \"seller-info-name\"}).find(\"a\").text\n",
    "    ad_address = soup.find(\"span\", attrs={\"itemtype\": \"http://schema.org/PostalAddress\"}).text\n",
    "    \n",
    "    page_meta[\"item\"] = ad_item_str\n",
    "    page_meta[\"id\"] = ad_item_id\n",
    "    page_meta[\"date\"] = ad_date\n",
    "    page_meta[\"seller\"] = clean_text(ad_seller)\n",
    "    page_meta[\"address\"] = clean_text(ad_address)\n",
    "    \n",
    "    return page_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(link):\n",
    "    link_html = requests.get(link).text\n",
    "    soup = BeautifulSoup(link_html, \"lxml\")\n",
    "    ad_title = soup.select(\"h1.title-info-title\")[0].text.strip()\n",
    "    \n",
    "    ad_description = soup.find(\"div\", attrs={\"class\": \"item-description\"}).find(\"div\")\n",
    "    ad_paragraphs = list(ad_description.findChildren(\"p\", recursive=False))\n",
    "    ad_text = []\n",
    "    ad_text = \"\\n\".join([par.text for par in ad_paragraphs])\n",
    "    \n",
    "    total_text = ad_title + ad_text\n",
    "    page_meta = extract_meta(link, soup)\n",
    "    return total_text, page_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    reg_tag = re.compile(\"<.*?>\")\n",
    "    reg_space = re.compile(\"\\s{2,}\")\n",
    "    new_text = html.unescape(raw_text)\n",
    "    new_text = reg_tag.sub(\"\", new_text)\n",
    "    new_text = reg_space.sub(\" \", new_text)\n",
    "    new_text = new_text.replace(\"\\xa0\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \".\" + os.sep + \"Avito_Beauty_Corpus\"\n",
    "if not os.path.exists(corpus):\n",
    "    os.mkdir(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_page(link, contents):\n",
    "    item = link[46:]\n",
    "    page_path = corpus + os.sep + item +  \".txt\"\n",
    "    with open(page_path, \"w\", encoding=\"utf-8\") as page_file:\n",
    "        page_file.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала отдельно соберём ссылки на страницы с товарами и сохраним их в отдельный файл:\n",
    "\n",
    "\n",
    "```python\n",
    "start_link = \"https://www.avito.ru/moskva/krasota_i_zdorove\"\n",
    "ready_links = []\n",
    "\n",
    "for i in range(1, 200):\n",
    "    # генерируем правильный адрес страницы, откуда будем таскать\n",
    "    # ссылки на объявления\n",
    "    if i > 1:\n",
    "        current_link = start_link + \"?p={}\".format(i)\n",
    "    else:\n",
    "        current_link = start_link\n",
    "    time.sleep(random.randint(1,20))\n",
    "    print(\"Parsing page: {}\".format(current_link))\n",
    "    # забираем ссылки на объявления со страницы\n",
    "    page = requests.get(current_link)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.text, \"lxml\")\n",
    "        item_hrefs = [a[\"href\"] for a in soup.select(\"a.item-description-title-link\")]\n",
    "        print(\"Found {} items\".format(len(item_hrefs)))\n",
    "        # собираем отдельно\n",
    "        for item_href in item_hrefs:\n",
    "            ready_link = \"https://www.avito.ru\" + item_href\n",
    "            ready_links.append(ready_link)\n",
    "    print(\"Done!\")\n",
    "```\n",
    "\n",
    "Я уже это сделала, но этот код можно снова перенести в ячейку, и он будет работать. А теперь уже будем обкачивать сами страницы с объявлениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./links.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    links = [link.strip() for link in f.readlines()]\n",
    "print(\"Начинаем собирать корпус из {} ссылок\".format(len(links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "meta_list = {\n",
    "    \"item\": [],\n",
    "    \"id\": [],\n",
    "    \"date\": [],\n",
    "    \"seller\": [],\n",
    "    \"address\": []\n",
    "}\n",
    "\n",
    "for item_link in links:\n",
    "    sleep_time = random.randint(1, 20)\n",
    "    print(\"Sleeping for {} sec.\".format(sleep_time))\n",
    "    time.sleep(sleep_time)\n",
    "    print(\"Parsing webpage: {}\".format(item_link))\n",
    "    try:\n",
    "        raw_text, meta = parse_page(item_link)\n",
    "        new_text = clean_text(raw_text)\n",
    "        save_page(item_link, new_text)\n",
    "        for col in meta:\n",
    "            meta_list[col].append(meta[col])\n",
    "        counter += 1\n",
    "        print(\"SUCCESS! Parsed {} pages\".format(counter))\n",
    "        break\n",
    "    except:\n",
    "        print(\"FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.DataFrame(meta_list)\n",
    "items_df.set_index(\"id\", inplace=True)\n",
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.to_csv(\"./items_meta.csv\", sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
