{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 1: _Индекс_\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Чтение файла \n",
    "- конструкция __with open__ (recommended)\n",
    "- конструкция __open + close__\n",
    "\n",
    "```python\n",
    "fpath = \"fpath.txt\"\n",
    "\n",
    "# одним массивом  \n",
    "with open(fpath, \"r\") as f:  \n",
    "    text = f.read() \n",
    "\n",
    "# по строкам, в конце каждой строки \\n  \n",
    "with open(fpath, \"r\") as f:   \n",
    "    text = f.readlines() \n",
    "\n",
    "# по строкам, без \\n   \n",
    "with open(fpath, \"r\") as f:   \n",
    "    text = f.read().splitlines() \n",
    "    \n",
    "# not recommended  \n",
    "file = open(txt_fpath, \"r\")  \n",
    "text = file.read()    \n",
    "file.close() \n",
    "```\n",
    "\n",
    "### Работа с файлами и папками\n",
    "#### os.path  \n",
    "Путь до файла:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# возвращает полный путь до папки/файла по имени файла / папки\n",
    "print(os.path.abspath(\"fpath.txt\"))\n",
    "\n",
    "# возвращает имя файла / папки по полному пути до него\n",
    "print(os.path.basename(\"/your/path/to/folder/with/fpath.txt\"))\n",
    "\n",
    "# проверить существование директории - True / False\n",
    "print(os.path.exists(\"your/path/to/any/folder/\"))\n",
    "```\n",
    "\n",
    "#### os.listdir  \n",
    "* Возвращает список файлов в данной директории:\n",
    "\n",
    "```python\n",
    "main_dir = \"/your/path/to/folder/with/folders/\"\n",
    "os.listdir(main_dir)\n",
    "```\n",
    "\n",
    "* Сделаем пути абсолютными, чтобы наш код не зависел от того, где лежит этот файл:\n",
    "```python\n",
    "[main_dir + fpath for fpath in os.listdir(main_dir)]\n",
    "```\n",
    "\n",
    "* Не забывайте исключать системные директории, такие как `.DS_Store`\n",
    "```python\n",
    "[main_dir + fpath for fpath in os.listdir(main_dir) if not \".DS_Store\" in fpath]\n",
    "```\n",
    "\n",
    "#### os.walk\n",
    "`root` — начальная директория  \n",
    "`dirs` — список поддиректорий (папок)   \n",
    "`files` — список файлов в этих поддиректориях  \n",
    "\n",
    "```python\n",
    "main_dir = \"/your/path/to/folder/with/folders/\"\n",
    "\n",
    "for root, dirs, files in os.walk(main_dir):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "```\n",
    "\n",
    "> __os.walk__ возвращает генератор. Это значит, что получить его элементы можно, только проитерировавшись по нему, но его легко можно превратить в `list` и увидеть все его значения\n",
    "\n",
    "```python\n",
    "list(os.walk(main_dir))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Обратный индекс \n",
    "\n",
    "Сам по себе обратный индекс не может осуществлять поиск, для этого необходимо добавить к нему определенную метрику. Это не совсем очевидная задача, поэтому немного отложим ее. А сейчас посмотрим, что полезного можно вытащить из индекса.    \n",
    "По сути, индекс — это информация о частоте встречаемости слова в каждом документе.   \n",
    "Из этого можно понять, например:\n",
    "\n",
    "1. какое слово является самым часто употребимым / редким\n",
    "2. какие слова встречаются всегда вместе. Так можно парсить твиттер, fb, форумы и отлавливать новые устойчивые выражения в речи\n",
    "3. какой документ является самым большим / маленьким (очень изощренный способ, когда есть `len`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 &emsp; __Задача__:  получить обратный индекс для коллекции документов.\n",
    "\n",
    "Перед этим постройте матрицу терм-документ и сделайте функцию булева поиска, которая по запросу будет возвращать 5 релевантных документов.   \n",
    "В качестве коллекции возьмите сценарий сезонов сериала Друзья. Одна серия — один документ.\n",
    "\n",
    "Скачать корпус можно [тут](https://yadi.sk/d/k_M7n63A3adGSz).\n",
    "\n",
    "Этапы:   \n",
    "\n",
    "1. получить коллекцию документов\n",
    "2. для каждого файла коллекции сделать необходимую на ваш взгляд предобработку\n",
    "3. получить матрицу терм-документ, написать функцию поиска по ней\n",
    "4. получить обратный индекс в виде словаря, где ключ — нормализованное слово, значение — список файлов, в которых это слово встречается\n",
    "5. вывести кусочек индекса в виде таблицы \n",
    "6. сделать анализ обратного индекса. Это задание принимается в виде кода и ответов на вопросы\n",
    "\n",
    "Напоминание:    \n",
    "> При итерации по списку вы можете помимо самого элемента получить его порядковый номер:\n",
    "```python \n",
    "for i, element in enumerate(your_list): \n",
    "    ... \n",
    "```    \n",
    "Иногда для получения элемента делают так — `your_list[i]`, старайтесь этого избегать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подгружаем файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"./Friends\"\n",
    "files_list = []\n",
    "\n",
    "### пройдитесь по всем папкам коллекции и соберите все пути .txt файлов\n",
    "for root, dirs, files in os.walk(main_dir):\n",
    "    for season_dir in dirs:\n",
    "        half_path = root + \"/\" + season_dir\n",
    "        files_list += [half_path + \"/\" + fpath for fpath in os.listdir(half_path) if not \".DS_Store\" in fpath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### _check : в коллекции должно быть 165 файлов\n",
    "len(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_punct = re.compile(\"[«–»—!\\$%&'()*+,./:;<=>?@^_`{|}~']*\")\n",
    "reg_num = re.compile(\"[0-9]+\")\n",
    "reg_latin = re.compile(\"[a-z]+\")\n",
    "reg_episode = re.compile(\"[0-9]x[0123456789-]+\")\n",
    "\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\ufeff\", \"\")\n",
    "    \n",
    "    text = reg_punct.sub(\"\", text)\n",
    "    text = reg_num.sub(\"\", text)\n",
    "    text = reg_latin.sub(\"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texts_ready = []\n",
    "lemmas_ready = []\n",
    "episodes = []\n",
    "reg_word = re.compile(\"[А-ЯЁа-яё]+(?:(?:-[А-ЯЁа-яё]+)*)?\")\n",
    "trash = set([\" \", \"-\", \"\\n\", \"\"])\n",
    "\n",
    "for script_path in files_list:\n",
    "    with open(script_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_raw = f.read()\n",
    "    \n",
    "    text_processed = preprocessing(text_raw)\n",
    "    tokens = re.findall(reg_word, text_processed)\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = mystem.lemmatize(token)\n",
    "        lemmas += [l for l in lemma[:-1] if l not in trash]\n",
    "    lemmas_ready.append(lemmas)\n",
    "    text_lemmatized = \" \".join(lemmas)\n",
    "    texts_ready.append(text_lemmatized)\n",
    "    \n",
    "    episode = re.search(reg_episode, script_path).group(0)\n",
    "    episodes.append(episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрица терм-документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>аа</th>\n",
       "      <th>ааа</th>\n",
       "      <th>ааааа</th>\n",
       "      <th>ааааааа</th>\n",
       "      <th>аааааау</th>\n",
       "      <th>ааааах</th>\n",
       "      <th>аарон</th>\n",
       "      <th>аббатство</th>\n",
       "      <th>абонемент</th>\n",
       "      <th>абрикос</th>\n",
       "      <th>...</th>\n",
       "      <th>ярмарка</th>\n",
       "      <th>ярость</th>\n",
       "      <th>ясмин</th>\n",
       "      <th>ясно</th>\n",
       "      <th>ясность</th>\n",
       "      <th>ясный</th>\n",
       "      <th>яхта</th>\n",
       "      <th>ящерица</th>\n",
       "      <th>ящик</th>\n",
       "      <th>ящичек</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2x08</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2x20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2x24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2x16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2x07</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14045 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         аа  ааа  ааааа  ааааааа  аааааау  ааааах  аарон  аббатство  \\\n",
       "episode                                                               \n",
       "2x08      0    0      0        0        0       0      0          0   \n",
       "2x20      0    0      0        0        0       0      0          0   \n",
       "2x24      0    0      0        0        0       0      0          0   \n",
       "2x16      0    0      0        0        0       0      0          0   \n",
       "2x07      0    0      0        0        0       0      0          0   \n",
       "\n",
       "         абонемент  абрикос   ...    ярмарка  ярость  ясмин  ясно  ясность  \\\n",
       "episode                       ...                                            \n",
       "2x08             0        0   ...          0       0      0     0        0   \n",
       "2x20             0        0   ...          0       0      0     0        0   \n",
       "2x24             0        0   ...          0       0      0     1        0   \n",
       "2x16             0        0   ...          0       0      0     0        0   \n",
       "2x07             0        0   ...          0       0      0     1        0   \n",
       "\n",
       "         ясный  яхта  ящерица  ящик  ящичек  \n",
       "episode                                      \n",
       "2x08         0     0        0     0       0  \n",
       "2x20         0     0        0     0       0  \n",
       "2x24         0     0        0     0       0  \n",
       "2x16         0     0        0     0       0  \n",
       "2x07         0     0        0     0       0  \n",
       "\n",
       "[5 rows x 14045 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_matrix = pd.DataFrame(cv.fit_transform(texts_ready).A, \n",
    "                               columns=cv.get_feature_names(), \n",
    "                               index=None)\n",
    "# столбец episodes будет индексом, чтобы мы понимали, о каком эпизоде речь\n",
    "term_doc_matrix[\"episode\"] = episodes\n",
    "term_doc_matrix = term_doc_matrix.set_index(term_doc_matrix[\"episode\"])\n",
    "del term_doc_matrix[\"episode\"]\n",
    "term_doc_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note: булев поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это простой поиск по одному токену\n",
    "def simple_boolean_search(query) -> list:\n",
    "    \"\"\"\n",
    "    Produces a Boolean search according with the term-document matrix\n",
    "    :return: list of first 5 relevant documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = term_doc_matrix.index[term_doc_matrix[query] != 0].tolist()\n",
    "        return res\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запросы \n",
    "input_text = [\n",
    "    \"Моника & Фиби & Рэйчел & Чендлер & Джои & Росс\",\n",
    "    \"(Моника ИЛИ Фиби) & Рэйчел & (Чендлер ИЛИ Джои) & Росс\", \n",
    "    \"(НЕ Моника) & Фиби & Рэйчел & Чендлер & Джои & (НЕ Росс)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обратный индекс\n",
    "\n",
    "Совет для построения обратного индекса: \n",
    "> В качестве словаря используйте `defaultdict` из модуля `collections`   \n",
    "Так можно избежать конструкции \n",
    "```python \n",
    "dict.setdefault(key, default=None) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(texts_ready, episodes) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    inv_index = defaultdict(dict)\n",
    "    for i, text in enumerate(texts_ready):\n",
    "        episode = episodes[i]\n",
    "        count = Counter(mystem.lemmatize(text))\n",
    "        for data in count.most_common():\n",
    "            lemma, res = data\n",
    "            if lemma not in trash:\n",
    "                inv_index[lemma][episode] = res\n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = inverted_index(texts_ready, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['а', 'а-ля', 'аа', 'ааа', 'ааааа']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(inv_index)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аналитика\n",
    "С помощью обратного индекса произведите следующую аналитику:  \n",
    "\n",
    "1) общая аналитика\n",
    "- какое слово является самым частотным?\n",
    "- какое самым редким?\n",
    "- какой набор слов есть во всех документах коллекции?\n",
    "\n",
    "2) частота встречаемости имен главных героев в каждом сезоне      \n",
    "- какой сезон был самым популярным у Чендлера? у Моники?   \n",
    "- кто из главных героев статистически самый популярный? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Самое частотное:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq = 0\n",
    "max_freq_word = \"\"\n",
    "for lemma in inv_index:\n",
    "    freq = sum(inv_index[lemma].values())\n",
    "    if freq > max_freq:\n",
    "        max_freq = freq\n",
    "        max_freq_word = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'я'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Самое редкое:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = max_freq\n",
    "min_freq_word = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in inv_index:\n",
    "    freq = sum(inv_index[lemma].values())\n",
    "    if freq < max_freq:\n",
    "        min_freq = freq\n",
    "        min_freq_word = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пожа'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_freq_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Есть везде:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_everywhere = []\n",
    "for lemma in inv_index:\n",
    "    if len(inv_index[lemma]) == 165:\n",
    "        present_everywhere.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(present_everywhere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Самые популярные сезоны:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_popular_season(character):\n",
    "    seasons = {str(num): 0 for num in range(1,8)}\n",
    "    character = character.lower().strip(\" \\n\")\n",
    "    character = mystem.lemmatize(character)[0]\n",
    "    for episode in inv_index[character]:\n",
    "        season = episode[0]\n",
    "        seasons[season] += inv_index[character][episode]\n",
    "    return max(seasons, key=seasons.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_season(\"Моника\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_season(\"Чендлер\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Самый популярный персонаж:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "росс\n"
     ]
    }
   ],
   "source": [
    "max_char_freq = 0\n",
    "most_popular_char = \"\"\n",
    "for character in [\"Рейчел\", \"Моника\", \"Фиби\", \"Джоуи\", \"Чендлер\", \"Росс\"]:\n",
    "    character = character.lower().strip(\" \\n\")\n",
    "    character = mystem.lemmatize(character)[0]\n",
    "    char_freq = sum(inv_index[character].values())\n",
    "    if char_freq > max_char_freq:\n",
    "        max_char_freq = char_freq\n",
    "        most_popular_char = character\n",
    "print(most_popular_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция ранжирования Okapi BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса есть общепринятая формула для ранжирования *Okapi best match 25* ([Okapi BM25](https://ru.wikipedia.org/wiki/Okapi_BM25)).    \n",
    "Пусть дан запрос $Q$, содержащий слова  $q_1, ... , q_n$, тогда функция BM25 даёт следующую оценку релевантности документа $D$ запросу $Q$:\n",
    "\n",
    "$$ score(D, Q) = \\sum_{i}^{n} \\text{IDF}(q_i)*\\frac{(k_1+1)*f(q_i,D)}{f(q_i,D)+k_1(1-b+b\\frac{|D|}{avgdl})} $$ \n",
    "где   \n",
    ">$f(q_i,D)$ - частота слова $q_i$ в документе $D$ (TF)       \n",
    "$|D|$ - длина документа (количество слов в нём)   \n",
    "*avgdl* — средняя длина документа в коллекции    \n",
    "$k_1$ и $b$ — свободные коэффициенты, обычно их выбирают как $k_1$=2.0 и $b$=0.75   \n",
    "$$$$\n",
    "$\\text{IDF}(q_i)$ есть обратная документная частота (IDF) слова $q_i$: \n",
    "$$\\text{IDF}(q_i) = \\log\\frac{N-n(q_i)+0.5}{n(q_i)+0.5},$$\n",
    ">> где $N$ - общее количество документов в коллекции   \n",
    "$n(q_i)$ — количество документов, содержащих $q_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 2.0\n",
    "b = 0.75\n",
    "\n",
    "N = 165\n",
    "avgdl = mean([len(doc) for doc in lemmas_ready])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(query):\n",
    "    n_q = len(inv_index[query])\n",
    "    frac = (N - n_q + 0.5)/(n_q + 0.5)\n",
    "    idf = log(frac)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача__:    \n",
    "напишите функцию, которая сортирует поисковую выдачу для любого входящего запроса согласно метрике *Okapi BM25*.    \n",
    "Выведите 10 первых результатов и их скор по запросу **рождественские каникулы**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim() -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "\n",
    "def get_search_result() -> list:\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
